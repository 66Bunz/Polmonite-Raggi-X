{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Polmonite Raggi-X.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/66Bunz/Polmonite-Raggi-X/blob/main/Polmonite_Raggi_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvY5wrjXYHOA"
      },
      "source": [
        "#**Deep Learning per rilevare la polmonite da immagini a raggi X**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBJ_lCQJax2u"
      },
      "source": [
        "Questo algoritmo identifica automaticamente se un paziente soffre o meno di polmonite osservando le radiografie del torace. Visto che sono in gioco le vite delle persone, questo algoritmo deve essere estremamente accurato."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T3ghY-DdqAL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBeNKqMLHGkA"
      },
      "source": [
        "## Importazione moduli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAhWl7DfdM0M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4e85156-2099-4197-8a72-9f5e294d811e"
      },
      "source": [
        "# Librerie generiche\n",
        "import os             # Si importa la libreria\n",
        "import numpy as np    # Si importa la libreria assegnandole il nome 'np'\n",
        "import pandas as pd   # Si importa la libreria assegnandole il nome 'pd'\n",
        "import random         # Si importa la libreria \n",
        "import cv2            # Si importa la libreria\n",
        "import matplotlib.pyplot as plt   # Si importa una funzione della libreria assegnandole il nome 'plt'\n",
        "!pip install requests\n",
        "import requests\n",
        "\n",
        "# !pip install requests\n",
        "# from urllib.request import urlretrieve\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "# Il comando seguente è un comando di shell che fa sì che i dati dei grafici siano mostrati nel Jupiter Notebook (Google Colaboratory lo è) (viene usato perché se no i grafici sarebbero delle singole finestre, che non sarebbe possibile mostrare in un notebook o da browser)\n",
        "%matplotlib inline\n",
        "\n",
        "# Librerie per il machine learning\n",
        "import keras.backend as K   # Si importa una funzione della sottolibreria backend di keras (backend è una sottolibreria di keras) assegnandole il nome 'K'\n",
        "from keras.models import Model, Sequential    # Si importano due funzioni della della sottolibreria models di keras\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization   # Si importano cinque funzioni della sottolibreria layers di keras\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPool2D, LeakyReLU, Activation    # Si importano altre cinque funzioni della sottolibreria layers di keras\n",
        "from keras.optimizer_v2 import adam\n",
        "from keras.preprocessing.image import ImageDataGenerator  # Si importa una funzione della sottolibreria image della sottolibreria preprocessing di keras\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping   # Si importano tre funzioni della sottolibreria callbacks di keras\n",
        "import tensorflow as tf   # Si importa la libreria assegnandole il nome 'tf'\n",
        "\n",
        "# Variabili per rendere riproducibile l'esperimento (parametri che rendono la casualità uguale per tutti)\n",
        "seed = 232    # Assegnazione di un valore ad una variabile\n",
        "np.random.seed(seed)    # Inserimento della variabile in una funzione come inizializzazione della libreria NumPy (np)\n",
        "tf.random.set_seed(seed)    # Inserimento della variabile in una funzione come inizializzazione della libreria TensorFlow (tf)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kahhqvAg599"
      },
      "source": [
        "*LIBRERIE*\n",
        "*   **OS**: Os serve per interagire con il sistema operativo (Operating System).\n",
        "*   **NumPy**: Numpy serve per l'analisi numerica (NumericalPython).\n",
        "*   **Pandas**: Pandas serve per manipolare e analizzare i dati.\n",
        "*   **Random**: Random serve per generare dei numeri semi-casuali (infatti per un dispositivo deterministico come un computer o algoritmo è impossibile generare valori randomici visto che darà ogni volta lo stesso output da una data condizione di partenza o stato iniziale, definiti nella sezione di definizione della variabile 'seed').\n",
        "*   **OpenCV (cv2)**: OpenCV serve per la computer vision.\n",
        "*   **Matplotlib**: Matplotlib serve per la creazione di grafici.\n",
        "*   **Keras**: Keras serve da interfaccia per il Machine Learning e le reti neurali (solitamente fornite da TensorFlow).\n",
        "*   **TensorFlow**: TensorFlow serve per sviluppare e addestrare modelli di Machine Learning.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjfFyg3EPpTB"
      },
      "source": [
        "url = \"https://storage.googleapis.com/kaggle-data-sets/17810/23812/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211031%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211031T120209Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=2ede3b4fc56fd330d0d0ae92cddd973efa55350326bd2dcdb46d602fcb8d55098035560ba21f515d3c9563aeaf7e232807199ebecc50f89c1a355ef79fdb2c871536009d732d5ca6670946d4160be82bad3e7b5a38e25a0f9c26cf5344ebf81faceef63bbb38267e48c3b5afb2d10110407874dbd1a41750c70c995faa9ea930b6275d678bd8cdcef82ced4b938fdf14e524aada1e5d41a5044639016b3c5350705cdfa2527e0fa017a026e04a3da4348c0c1b3bfc301101e8515801ba477a4dbe0a033a29ecf9fbf313f992ef572ba9307f8719d831376b705c597bae8327fab60b731f4bf95793e9d2c1242ecbe64acfbfe53d1b5c1584f44578a1115e0a0b\"\n",
        "dst = \"archive.zip\"\n",
        "urllib.request.urlretrieve(url, dst)\n",
        "\n",
        "html = open(dst)\n",
        "print(html)\n",
        "html.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teyE-2gmH3aq"
      },
      "source": [
        "## Montaggio del drive per recuperare le immagini di addestramento, di test e di validazione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjEMq1UD9Svx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ff0d84f-c912-4c4d-955a-d7c6a547dfa4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgKYicH9HqIJ"
      },
      "source": [
        "## Assegnazione ad una variabile il percorso della cartella con tutte le immagini di addestramento, di test e di validazione"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDo6H8bBX_1-"
      },
      "source": [
        "input_path = '/content/drive/MyDrive/Colab Notebooks/Compito X-Ray Polmonite/chest_xray/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMcilSW9IQaz"
      },
      "source": [
        "## Visualizzazione di immagini di esempio prese dai tre dataset di allenamento, valutazione e test con e senza polmonite\n",
        "\n",
        "> Blocco con rientro\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbFEk7F9ICBq"
      },
      "source": [
        "# Mostra 3 paia di immagini prese dai tre dataset di allenamento, valutazione e test senza e con polmonite\n",
        "fig, ax = plt.subplots(2, 3, figsize=(15, 7))\n",
        "ax = ax.ravel()\n",
        "plt.tight_layout()\n",
        "\n",
        "for i, _set in enumerate(['train', 'val', 'test']):\n",
        "    set_path = input_path+_set\n",
        "    ax[i].imshow(plt.imread(set_path+'/NORMAL/'+os.listdir(set_path+'/NORMAL')[0]), cmap='gray')\n",
        "    ax[i].set_title('Set: {} - Condizione: Sano'.format(_set))\n",
        "    ax[i+3].imshow(plt.imread(set_path+'/PNEUMONIA/'+os.listdir(set_path+'/PNEUMONIA')[0]), cmap='gray')\n",
        "    ax[i+3].set_title('Set: {} - Condizione: Polmonite'.format(_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flvbXzljKayx"
      },
      "source": [
        "## Definizioni per l'addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_M9mMVvYKKF"
      },
      "source": [
        "# Distribuzione dei dataset\n",
        "for _set in ['train', 'val', 'test']:\n",
        "    n_normal = len(os.listdir(input_path + _set + '/NORMAL'))\n",
        "    n_infect = len(os.listdir(input_path + _set + '/PNEUMONIA'))\n",
        "    print('Set: {}, immagini sane: {}, immagini polmoniti: {}'.format(_set, n_normal, n_infect))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XNBauRXYMO-"
      },
      "source": [
        "def process_data(img_dims, batch_size):\n",
        "    # Definizione dei dati delle generazioni di allenamento e di test\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, zoom_range=0.3, vertical_flip=True)\n",
        "    test_val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "    \n",
        "    # Definizione delle generazioni di allenamento e di test\n",
        "    train_gen = train_datagen.flow_from_directory(\n",
        "    directory=input_path+'train', \n",
        "    target_size=(img_dims, img_dims), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary', \n",
        "    shuffle=True)\n",
        "\n",
        "    test_gen = test_val_datagen.flow_from_directory(\n",
        "    directory=input_path+'test', \n",
        "    target_size=(img_dims, img_dims), \n",
        "    batch_size=batch_size, \n",
        "    class_mode='binary', \n",
        "    shuffle=True)\n",
        "    \n",
        "    # Creazione di previsioni del set di test, utile per avere una matrice di confusione/confusion matrix (tabella usata per valutare le performance di un algoritmo)\n",
        "    test_data = []\n",
        "    test_labels = []\n",
        "\n",
        "    for cond in ['/NORMAL/', '/PNEUMONIA/']:\n",
        "        for img in (os.listdir(input_path + 'test' + cond)):\n",
        "            img = plt.imread(input_path+'test'+cond+img)\n",
        "            img = cv2.resize(img, (img_dims, img_dims))\n",
        "            img = np.dstack([img, img, img])\n",
        "            img = img.astype('float32') / 255\n",
        "            if cond=='/NORMAL/':\n",
        "                label = 0\n",
        "            elif cond=='/PNEUMONIA/':\n",
        "                label = 1\n",
        "            test_data.append(img)\n",
        "            test_labels.append(label)\n",
        "        \n",
        "    test_data = np.array(test_data)\n",
        "    test_labels = np.array(test_labels)\n",
        "    \n",
        "    return train_gen, test_gen, test_data, test_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49pLTm7nYQx2"
      },
      "source": [
        "# Variabili utilizzate per l'addestramento della macchina\n",
        "img_dims = 150\n",
        "epochs = 2\n",
        "batch_size = 32\n",
        "\n",
        "# Elaborazione dei dati\n",
        "train_gen, test_gen, test_data, test_labels = process_data(img_dims, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHIT-wh5E8Vz"
      },
      "source": [
        "CONVOLUZIONE: operazione tra due funzioni che restituisce una terza funzione che esprime come la forma di una è modificata dall'altra funzione iniziale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA-YQ57_YTD1"
      },
      "source": [
        "# Livello di input\n",
        "inputs = Input(shape=(img_dims, img_dims, 3))\n",
        "\n",
        "# Primo blocco di convoluzione\n",
        "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(inputs)\n",
        "x = Conv2D(filters=16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Secondo blocco di convoluzione\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=32, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Terzo blocco di convoluzione\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=64, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Quarto blocco di convoluzione\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=128, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "\n",
        "# Quinto blocco di convoluzione\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = SeparableConv2D(filters=256, kernel_size=(3, 3), activation='relu', padding='same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = MaxPool2D(pool_size=(2, 2))(x)\n",
        "x = Dropout(rate=0.2)(x)\n",
        "\n",
        "# Livello FC (Fully Connected, ovvero che collega tutti i neuroni di un livello con quelli di un'altro)\n",
        "x = Flatten()(x)\n",
        "x = Dense(units=512, activation='relu')(x)\n",
        "x = Dropout(rate=0.7)(x)\n",
        "x = Dense(units=128, activation='relu')(x)\n",
        "x = Dropout(rate=0.5)(x)\n",
        "x = Dense(units=64, activation='relu')(x)\n",
        "x = Dropout(rate=0.3)(x)\n",
        "\n",
        "# Livello di output\n",
        "output = Dense(units=1, activation='sigmoid')(x)\n",
        "\n",
        "# Creazione del modello e compilazione\n",
        "model = Model(inputs=inputs, outputs=output)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Callbacks (funzioni che consentono di avere una visuale del modello durante l'allenamento)\n",
        "checkpoint = ModelCheckpoint(filepath='best_weights.hdf5', save_best_only=True, save_weights_only=True)   # Salvataggio del modello dopo ogni epoca\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=2, verbose=2, mode='max')    # Interrompe l'allenamento se il modello smette di migliorare\n",
        "early_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')   # Interrompe l'addestramento in caso di sovradattamento (quando la funzione impara troppo bene solo i valori di addestramento)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXqQF3lDGpf-"
      },
      "source": [
        "## Addestramento del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1C77ZguYWnu"
      },
      "source": [
        "# Addestramento del modello\n",
        "hist = model.fit_generator(\n",
        "           train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
        "           epochs=epochs, validation_data=test_gen, \n",
        "           validation_steps=test_gen.samples // batch_size, callbacks=[checkpoint, lr_reduce])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFsQBP48KQq5"
      },
      "source": [
        "## Visualizzazione dei grafici di accuratezza e perdita"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XE6PNVMYYTe"
      },
      "source": [
        "# Mostra grafici di accuratezza e perdita\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
        "ax = ax.ravel()\n",
        "\n",
        "for i, met in enumerate(['accuracy', 'loss']):\n",
        "    ax[i].plot(hist.history[met])\n",
        "    ax[i].plot(hist.history['val_' + met])\n",
        "    ax[i].set_title('Modello {}'.format(met))\n",
        "    ax[i].set_xlabel('epoche')\n",
        "    ax[i].set_ylabel(met)\n",
        "    ax[i].legend(['train', 'val'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeNzqpwsKBwf"
      },
      "source": [
        "## Test del modello allenato con immagini di test che l'algoritmo non ha ancora visto"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFvgd9nKYZ82"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "preds = model.predict(test_data)\n",
        "\n",
        "acc = accuracy_score(test_labels, np.round(preds))*100\n",
        "cm = confusion_matrix(test_labels, np.round(preds))\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "# Stampa dei risultati dei parametri\n",
        "print('MATRICE DI CONFUSIONE ------------------')\n",
        "print(cm)\n",
        "print('\\nPARAMETRI DI TEST ----------------------')\n",
        "precision = tp/(tp+fp)*100\n",
        "recall = tp/(tp+fn)*100\n",
        "print('Accuratezza: {}%'.format(acc))\n",
        "print('Precisione: {}%'.format(precision))\n",
        "print('Sensibilità: {}%'.format(recall))    # Proporzione dei veri positivi\n",
        "print('F1-score: {}'.format(2*precision*recall/(precision+recall)))   # F1-score è una media armonica di precisione e sensibilità\n",
        "print('\\PARAMETRI DI ADDESTRAMENTO ----------------------')\n",
        "print('Accuratezza modello addestrato: {}'.format(np.round((hist.history['accuracy'][-1])*100, 2)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}